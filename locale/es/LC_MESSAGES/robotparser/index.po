#
msgid ""
msgstr ""
"Project-Id-Version: Python Module of the Week 2.1.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2015-10-06 13:35-0300\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../PyMOTW/robotparser/index.rst:3
msgid "robotparser -- Internet spider access control"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:8
msgid "Parse robots.txt file used to control Internet spiders"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:9
msgid "2.1.3 and later"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:11
msgid ""
":mod:`robotparser` implements a parser for the ``robots.txt`` file format, "
"including a simple function for checking if a given user agent can access a "
"resource.  It is intended for use in well-behaved spiders or other crawler "
"applications that need to either be throttled or otherwise restricted."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:15
msgid ""
"The :mod:`robotparser` module has been renamed :mod:`urllib.robotparser` in "
"Python 3.0. Existing code using :mod:`robotparser` can be updated using "
"2to3."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:19
msgid "robots.txt"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:21
msgid ""
"The ``robots.txt`` file format is a simple text-based access control system "
"for computer programs that automatically access web resources (\"spiders\", "
"\"crawlers\", etc.).  The file is made up of records that specify the user "
"agent identifier for the program followed by a list of URLs (or URL "
"prefixes) the agent may not access."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:23
msgid "This is the ``robots.txt`` file for ``http://www.doughellmann.com/``:"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:28
msgid ""
"It prevents access to some of the expensive parts of my site that would "
"overload the server if a search engine tried to index them.  For a more "
"complete set of examples, refer to `The Web Robots Page`_."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:31
msgid "Simple Example"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:33
msgid ""
"Using the data above, a simple crawler can test whether it is allowed to "
"download a page using the ``RobotFileParser``'s ``can_fetch()`` method."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:39
msgid ""
"The URL argument to ``can_fetch()`` can be a path relative to the root of "
"the site, or full URL."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:67
msgid "Long-lived Spiders"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:69
msgid ""
"An application that takes a long time to process the resources it downloads "
"or that is throttled to pause between downloads may want to check for new "
"``robots.txt`` files periodically based on the age of the content it has "
"downloaded already.  The age is not managed automatically, but there are "
"convenience methods to make tracking it easier."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:75
msgid ""
"This extreme example downloads a new ``robots.txt`` file if the one it has "
"is more than 1 second old."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:100
msgid ""
"A \"nicer\" version of the long-lived application might request the "
"modification time for the file before downloading the entire thing.  On the "
"other hand, ``robots.txt`` files are usually fairly small, so it isn't that "
"much more expensive to just grab the entire document again."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:106
msgid "`robotparser <http://docs.python.org/library/robotparser.html>`_"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:106
msgid "The standard library documentation for this module."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:108
msgid "`The Web Robots Page`_"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:109
msgid "Description of robots.txt format."
msgstr ""
